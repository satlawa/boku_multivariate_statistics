---
author: "Philipp Satlawa - h0640348"
date: "27/05/2021"
title: "Multivariate Statistics - Exercise 4"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This document contains the answered questions of exercise 4 of the course "Multivariate Statistics".

***

# Principal Component Analysis (PCA)

## 1. install and load the packages - load and summerize the data
```{r}
# import necessary libraries
library(ISLR)
library(cvTools)
library(pls)

# load data
data(OJ)

# explore data
help(OJ)
head(OJ)
str(OJ)
summary(OJ)

# convert categorical variables to factors
OJ$StoreID<-as.factor(OJ$StoreID)
OJ$STORE<-as.factor(OJ$STORE)
OJ$SpecialCH<-as.factor(OJ$SpecialCH)
OJ$SpecialMM<-as.factor(OJ$SpecialMM)
```
The dataset `OC` contains 1070 records and 18 attributes. The records represent purchases of orange juice for two brands (`CH` and `MM`). All except for two variables (`Purchase` and `Store7`) are *numerical*, however the variables `StoreID`, `STORE`, `SpecialCH` and `SpecialMM` are encoded as numerical but are in reality categorical. Hence, we change the representation of these variables from *numerical* to *factor*. Furthermore, some attribures seem to contain redundant information, such as `StoreID` and `STORE`.


## 2. split data into train and test set
```{r}
# set seed
set.seed(3333)

# create indices and shuffle randomly
idx = sample(dim(OJ)[1])

# set train split
train_split = 0.75

# split dataset in train and test
train = OJ[idx[1:(dim(OJ)[1]*train_split)],]
test = OJ[idx[((dim(OJ)[1]*train_split)+1):dim(OJ)[1]],]

# print class proportions of train and test
print(oj_prop <- table(OJ$Purchase))
print(train_prop <- table(train$Purchase))
print(test_prop <- table(test$Purchase))

# calculate ratio CH/MM
print(oj_prop["CH"]/oj_prop["MM"])
print(train_prop["CH"]/train_prop["MM"])
print(test_prop["CH"]/test_prop["MM"])
```
The class proportions of the train-set and test-set are balanced (although not the same, the CH/MM ratio is 1.57 in train versus 1.54 in test and 1.57 in the entire data set). An imbalance in the class-proportions of the two subsets can result in a model that performs worse than trained on a balanced class-proportions dataset. The imbalance of the classes in the subsets is directly linked to the prior probability, if the classifier takes the prior probability into account and the subsets are imbalanced the trained model is biased towards one class and the prediction is worse. In the worst case, we can have a split where one class (e.g. CH) is just in the training set and the other class (e.g. MM) just in the test set. This would result in a model that cannot predict class number two (e.g. MM) since there was no data in the training set. 


## 3. LDA
```{r}
# load package
require(MASS)

#LDA
lda_model_cv <- lda(Purchase ~ ., data = train, CV = TRUE)

# confusion matrix of the cross-validated LDA model
(table(prediction = lda_model_cv$class, truth = train[, "Purchase"]))
```
While training the model we encounter collinearity of some variables. This means at least two variables are strongly correlated hence contain the same information and are therefore redundant. To obtain a better model it is advisable to remove these variables.


## 4. select variables for predicting 
```{r}
# plot time/location predictors
pairs(OJ[,c("StoreID", "STORE", "Store7")])

# plot price predictors
pairs(OJ[,c("PriceCH", "SalePriceCH", "PriceDiff", "DiscCH", "PctDiscMM", 
            "SpecialCH", "ListPriceDiff")])

# clean data sets
OJ_cl <- OJ[,c("Purchase", "WeekofPurchase", "StoreID", "PriceCH", "PriceMM", 
               "DiscCH", "DiscMM", "LoyalCH")]
train_cl = train[,c("Purchase", "WeekofPurchase", "StoreID", "PriceCH",
                    "PriceMM", "DiscCH", "DiscMM", "LoyalCH")]
test_cl = test[,c("Purchase", "WeekofPurchase", "StoreID", "PriceCH", 
                  "PriceMM", "DiscCH", "DiscMM", "LoyalCH")]
```
**Time/Location Predictors**

 * In the subgroup *time/location* the variables `StoreID` and `STORE` are basically the same the only difference is the representation (coding). 
 * The variable `Store7` is contained in the variables `StoreID` and `STORE`, hence it is redundant.

**Price Predictors**

There are several attributes with relationships the subgroup describing the *price* that are containing the same information.

  * `SalePriceXX` = `PriceXX` - `DiscXX`
  * `DiscXX` = `PriceXX` * `PctDiscXX`
  * `PriceDiff` = `SalePriceMM` - `SalePriceCH`
  * `ListPriceDiff` = `PriceMM` - `PriceCH`

Since the variables can be explained through the above described relationships the two variables `PriceXX` and `DiscXX` contain all the necessary information.

To conclude, many of the variables of the original dataset contain the same information, hence the following attributes are sufficient to train a model without loosing any information `WeekofPurchase`, `StoreID`, `PriceCH`, `PriceMM`, `DiscCH`, `DiscMM`, `LoyalCH`.


## 5. LDA on train set
```{r}
#LDA
lda_model_cv <- lda(Purchase ~ ., data = train_cl, CV = TRUE)

# confusion matrix of the cross-validated LDA model
(cm_train <- table(prediction = lda_model_cv$class, truth = train_cl[, "Purchase"]))

# total observations, correct and misclassified
n <- dim(train_cl)[1]
correct <- sum(diag(cm_train))
mis <- n - correct

# apparent error rate
(APER <- mis/n)
```
By applying LDA on the cleaned train set we obtain an *apparent error rate* of 0.178 for the classifier.


## 5. LDA on test set
```{r}
#LDA
lda_model <- lda(Purchase ~ ., data = train_cl, CV = FALSE)

# predict test set cases
preds_test <- predict(lda_model, newdata = test_cl)

# confusion matrix of the final LDA model
(cm_test <- table(predictions = preds_test$class, truth = test_cl[, "Purchase"]))

# total observations, correct and misclassified
n <- dim(test_cl)[1]
correct <- sum(diag(cm_test))
mis <- n - correct

# apparent error rate
(APER <- mis/n)
```
After retraining the model on the cleaned train set we apply the cleaned test set on the final model and receive an *apparent error rate* of 0.176. This result is matching with the outcome of the previously trained model `lda_model_cv`.


***

# Regression

## 1. load data
```{r}
# load data
data(Boston)

# explore data
help(Boston)
head(Boston)
str(Boston)
summary(Boston)

# missing values
which(is.na(Boston))

# convert categorical variables to factors
Boston$chas <- as.factor(Boston$chas)
Boston$rad <- as.factor(Boston$rad)
```
The dataset `Boston` contains 506 records and 14 attributes that represent the housing values in suburbs of Boston. The majority of variables are represented as continuous numbers, just the variables `chas` and `rad` are represented as discrete numbers (integers) because they are actually categorical variables. The attribute `rad` is additionally an ordinal variable.


## 2. split data into train and test set
```{r}
# set seed
set.seed(3333)

# create indices and shuffle randomly
idx = sample(dim(Boston)[1])

# set train split
train_split = 2/3

# split dataset in train and test
train = Boston[idx[1:(dim(Boston)[1]*train_split)],]
test = Boston[idx[((dim(Boston)[1]*train_split)+1):dim(Boston)[1]],]

# print dimensions of train and test
dim(train)
dim(test)

# print class proportions of train set
print((mean(train$medv)))
print((sd(train$medv)))
# print class proportions of test set
print((mean(test$medv)))
print((sd(test$medv)))

```
By comparing the mean of the target variable `medv` in both the train and test set we get similar numbers 22.4 and 22.8 respectively, which indicates a good split between the train and test set (at least of the target variable). The higher standard deviation in the test set also makes sense due to the smaller number of records contained in the test set the standard deviation is expected to be higher compared to the train set.
The model we want to create for predicting the house prices shall be as simple as possible, hence flexible for adapting to unknown data, however as complex as necessary due to the fact that we do not have the future data and the best guess is looking into the data of the past. Too complex models might fit the training data perfectly however they often perform poorly on unseen data (overfitting) therefore we hold out a part of our data (test set) to be able to validate the model.


## 3. simple regression model
```{r}
# linear model
lm_simp <- lm(medv ~ rm, data = train)

# scatterplot with regression line
plot(train[c("rm", "medv")], col = rgb(0,0.2,1,0.4), pch = 20,
      main = "Simple Linear Regression",
      xlab = "Average number of rooms per dwelling",
      ylab = "Median value of owner-occupied homes [$1000]")
abline(lm_simp, lwd = 1, col = "red")
```
After calculating the simple regression model, we can see that there is a correlation between the two attributes `medv` and `rm`. Nonetheless, there are many data points that cannot be explained by just the linear relationship between `medv` and `rm`. In particular there are some outliers, that are high value homes with a small average number of rooms per dwelling and cannot be explained by this simple regression model.


## 4. RMSE
```{r}
# predict train set
preds_simp <- predict(lm_simp)
# calculate RMSE on train set
sqrt(mean((preds_simp - train[, "medv"])^2))

# use cross validation on train set
cvFit(lm_simp, data = train, y = train$medv, K = 10, seed = 3333)
```
The results show a slightly higher RMSE 6.87 calculated on a 10-fold cross validated train set compared to the RMSE of 6.83 obtained on the train set without cross validation.


## 5. Partial Least Squares (PLS)
```{r}
# make this example reproducible
set.seed(3333)
# PLS on train set with 10-fold CV
pls_model <- plsr(medv ~ ., data = train, ncomp = 10, validation = "CV")
# print summary
summary(pls_model)
```
From the results of the PLS without scaling the data, we can deduct that the accuracy of the model increases with the number of components used. Considering the steady increases we would use at lest 7 components that can explain 60.3 % of `medv`s variance. Certainly, it is also possible to to use 10 components to reduce the RMSE by ruffly 0.6 and get a model that can explain 66.7% of `medv`s variance.


## 6. prediction error on the test data
```{r}
# Performance on test set for 7-component PLS model
preds_pls_test <- predict(pls_model, newdata = test, ncomp = 7)

# calculate test RMSE
sqrt(mean((preds_pls_test - test[, "medv"])^2))
```
The PLS model with 7 components achieves a RMSE of 5.25 on the test data.


## 7. scale the data and apply Partial Least Squares (PLS)
```{r}
# make this example reproducible
set.seed(3333)
# PLS on train set with 10-fold CV
pls_model_scaled <- plsr(medv ~ ., data = train, ncomp = 10, validation = "CV", 
                         scale = TRUE)
summary(pls_model_scaled)

# Performance on test set for scaled 4-component PLS model
preds_pls_scaled_test <- predict(pls_model_scaled, newdata = test, ncomp = 5)
# calculate test RMSE
sqrt(mean((preds_pls_scaled_test - test[, "medv"])^2))
```
Training the model on scaled data increased the models performance and reduced the number of essential components to 4. With 4 components we are able to explain 68.3 % of `medv`s variance and obtain a RMSE 5.31 on the train set. 
If we now compare the non-scaled PLS-model with the scaled PLS-model, we can can clearly see the superiority of the scaled PLS-model, where we can obtain with 4 components a RMSE of 4.37 on the test set, compared to a RMSE of 5.25 the test set of the non-scaled PLS-model.


## 8. Visualize performance
```{r fig, fig.height = 5, fig.width = 5, fig.align = "center"}
# Plot results
plot(test[, "medv"], preds_pls_scaled_test, col = rgb(0,0.2,1,0.4), pch = 20,
      xlim = c(0,55), ylim = c(0,55),
      main = "Measured versus Predicted Values",
      xlab = "Measured Median Value [$1000]", ylab = "Predicted Median Value [$1000]")
#abline(lm_simp, lwd = 1, col = "red")
```
In the plot we can see the model performing quite well until the median home value of \$30000. Homes with a measured median home value above \$30000 are being undervalued by the model. Thereby undervaluing the homes with the highest measured median home value (above \$40000) the most. However, for most cases the model predicts good estimates and can be deployed with the constraint that more valuable homes should be measured.

***